{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4506YZ5Boz6Lpl9rkH1fv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreya11020/neural_network_from_scratch/blob/main/neuralnetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "L = 3\n",
        "n = [2, 3, 3, 1]\n",
        "W1 = np.random.randn(n[1], n[0])\n",
        "W2 = np.random.randn(n[2], n[1])\n",
        "W3 = np.random.randn(n[3], n[2])\n",
        "b1 = np.random.randn(n[1], 1)\n",
        "b2 = np.random.randn(n[2], 1)\n",
        "b3 = np.random.randn(n[3], 1)\n",
        "\n",
        "def prepare_data():\n",
        "  X = np.array([\n",
        "      [150, 70],\n",
        "      [254, 73],\n",
        "      [312, 68],\n",
        "      [120, 60],\n",
        "      [154, 61],\n",
        "      [212, 65],\n",
        "      [216, 67],\n",
        "      [145, 67],\n",
        "      [184, 64],\n",
        "      [130, 69]\n",
        "  ])\n",
        "  y = np.array([0,1,1,0,0,1,1,0,1,0])\n",
        "  m = 10\n",
        "  A0 = X.T\n",
        "  Y = y.reshape(n[L], m)\n",
        "\n",
        "  return A0, Y, m\n",
        "\n",
        "def cost(y_hat, y):\n",
        "  \"\"\"\n",
        "  y_hat should be a n^L x m matrix\n",
        "  y should be a n^L x m matrix\n",
        "  \"\"\"\n",
        "  # 1. losses is a n^L x m\n",
        "  losses = - ( (y * np.log(y_hat)) + (1 - y)*np.log(1 - y_hat) )\n",
        "\n",
        "  m = y_hat.reshape(-1).shape[0]\n",
        "\n",
        "  # 2. summing across axis = 1 means we sum across rows,\n",
        "  #   making this a n^L x 1 matrix\n",
        "  summed_losses = (1 / m) * np.sum(losses, axis=1)\n",
        "\n",
        "  # 3. unnecessary, but useful if working with more than one node\n",
        "  #   in output layer\n",
        "  return np.sum(summed_losses)\n",
        "\n",
        "def g(z):\n",
        "  return 1 / (1 + np.exp(-1 * z))\n",
        "\n",
        "def feed_forward(A0):\n",
        "  # layer 1 calculations\n",
        "  Z1 = W1 @ A0 + b1\n",
        "  A1 = g(Z1)\n",
        "\n",
        "  # layer 2 calculations\n",
        "  Z2 = W2 @ A1 + b2\n",
        "  A2 = g(Z2)\n",
        "\n",
        "  # layer 3 calculations\n",
        "  Z3 = W3 @ A2 + b3\n",
        "  A3 = g(Z3)\n",
        "\n",
        "  cache = {\n",
        "      \"A0\": A0,\n",
        "      \"A1\": A1,\n",
        "      \"A2\": A2\n",
        "  }\n",
        "\n",
        "  return A3, cache"
      ],
      "metadata": {
        "id": "VuVjf2Kxqmd-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A0,Y,m = prepare_data()\n"
      ],
      "metadata": {
        "id": "Cc96lXrGqz3-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop_layer_3(y_hat, Y , m , A2, W3):\n",
        "  A3 = y_hat\n",
        "\n",
        "  # step 1. calculate dC/dZ3 using shorthand we derived earlier\n",
        "  dC_DZ3 = (1/m)*(A3-Y)\n",
        "\n",
        "\n",
        "  #step2. calculate dC/dW3 = dC/dZ3*dZ3/dW3\n",
        "  # we matrix multiply dC/dZ3 with (dZ3/dW3)^T\n",
        "\n",
        "  dZ3_dW3 = A2\n",
        "  assert dZ3_dW3.shape == (n[2],m)\n",
        "\n",
        "  dC_dW3 = dC_dZ3 @ dZ3_dW3.T\n",
        "  assert dC_dW3.shape == (n[3], n[2])\n",
        "  assert dC_dZ3.shape == (n[3],m) # Moved this assertion after calculation of dC_dZ3\n",
        "\n",
        "  # step 3. calculate dC/db3 = np.sum(dC/dZ3, axis=1, keepdims=True)\n",
        "  dC_db3 = np.sum(dC_dZ3, axis=1, keepdims=True)\n",
        "  assert dC_db3.shape == (n[3], 1)\n",
        "\n",
        "  # step 4. calculate propagator dC/dA2 = dC/dZ3 * dZ3/dA2\n",
        "  dZ3_dA2 = W3\n",
        "  dC_dA2 = W3.T @ dC_dZ3\n",
        "  assert dC_dA2.shape == (n[2], m)\n",
        "\n",
        "  return dC_dW3, dC_db3, dC_dA2"
      ],
      "metadata": {
        "id": "mKeQSSJgq9gs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop_layer_2(propagator_dC_dA2, A1, A2, W2):\n",
        "\n",
        "  # step 1. calculate dC/dZ2 = dC/dA2 * dA2/dZ2\n",
        "\n",
        "  # use sigmoid derivation to arrive at this answer:\n",
        "  #   sigmoid'(z) = sigmoid(z) * (1 - sigmoid(z))\n",
        "  #     and if a = sigmoid(z), then sigmoid'(z) = a * (1 - a)\n",
        "  dA2_dZ2 = A2 * (1 - A2)\n",
        "  dC_dZ2 = propagator_dC_dA2 * dA2_dZ2\n",
        "  assert dC_dZ2.shape == (n[2], m)\n",
        "\n",
        "\n",
        "  # step 2. calculate dC/dW2 = dC/dZ2 * dZ2/dW2\n",
        "  dZ2_dW2 = A1\n",
        "  assert dZ2_dW2.shape == (n[1], m)\n",
        "\n",
        "  dC_dW2 = dC_dZ2 @ dZ2_dW2.T\n",
        "  assert dC_dW2.shape == (n[2], n[1])\n",
        "\n",
        "  # step 3. calculate dC/db2 = np.sum(dC/dZ2, axis=1, keepdims=True)\n",
        "  dC_db2 = np.sum(dC_dW2, axis=1, keepdims=True)\n",
        "  assert dC_db2.shape == (n[2], 1)\n",
        "\n",
        "  # step 4. calculate propagator dC/dA1 = dC/dZ2 * dZ2/dA1\n",
        "  dZ2_dA1 = W2\n",
        "  dC_dA1 = dZ2_dA1.T @ dC_dZ2\n",
        "  assert dC_dA1.shape == (n[2], m)\n",
        "\n",
        "  return dC_dW2, dC_db2, dC_dA1\n",
        "\n",
        "def backprop_layer_1(propagator_dC_dA1, A1, A0, W1):\n",
        "\n",
        "  # step 1. calculate dC/dZ1 = dC/dA1 * dA1/dZ1\n",
        "\n",
        "  # use sigmoid derivation to arrive at this answer:\n",
        "  #   sigmoid'(z) = sigmoid(z) * (1 - sigmoid(z))\n",
        "  #     and if a = sigmoid(z), then sigmoid'(z) = a * (1 - a)\n",
        "  dA1_dZ1 = A1 * (1 - A1)\n",
        "  dC_dZ1 = propagator_dC_dA1 * dA1_dZ1\n",
        "  assert dC_dZ1.shape == (n[1], m)\n",
        "\n",
        "\n",
        "  # step 2. calculate dC/dW1 = dC/dZ1 * dZ1/dW1\n",
        "  dZ1_dW1 = A0\n",
        "  assert dZ1_dW1.shape == (n[0], m)\n",
        "\n",
        "  dC_dW1 = dC_dZ1 @ dZ1_dW1.T\n",
        "  assert dC_dW1.shape == (n[1], n[0])\n",
        "\n",
        "  # step 3. calculate dC/db1 = np.sum(dC/dZ1, axis=1, keepdims=True)\n",
        "  dC_db1 = np.sum(dC_dW1, axis=1, keepdims=True)\n",
        "  assert dC_db1.shape == (n[1], 1)\n",
        "\n",
        "  return dC_dW1, dC_db1"
      ],
      "metadata": {
        "id": "tCQ0P8cLuvuv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  # must use global keyword in order to modify global variables\n",
        "  global W3, W2, W1, b3, b2, b1\n",
        "\n",
        "  epochs = 1000 # training for 1000 iterations\n",
        "  alpha = 0.1 # set learning rate to 0.1\n",
        "  costs = [] # list to store costs\n",
        "\n",
        "  for e in range(epochs):\n",
        "    # 1. FEED FORWARD\n",
        "    y_hat, cache = feed_forward(A0)\n",
        "\n",
        "    # 2. COST CALCULATION\n",
        "    error = cost(y_hat, Y)\n",
        "    costs.append(error)\n",
        "\n",
        "    # 3. BACKPROP CALCULATIONS\n",
        "\n",
        "    dC_dW3, dC_db3, dC_dA2 = backprop_layer_3(\n",
        "        y_hat,\n",
        "        Y,\n",
        "        m,\n",
        "        A2= cache[\"A2\"],\n",
        "        W3=W3\n",
        "    )\n",
        "\n",
        "    dC_dW2, dC_db2, dC_dA1 = backprop_layer_2(\n",
        "        propagator_dC_dA2=dC_dA2,\n",
        "        A1=cache[\"A1\"],\n",
        "        A2=cache[\"A2\"],\n",
        "        W2=W2\n",
        "    )\n",
        "\n",
        "    dC_dW1, dC_db1 = backprop_layer_1(\n",
        "        propagator_dC_dA1=dC_dA1,\n",
        "        A1=cache[\"A1\"],\n",
        "        A0=cache[\"A0\"],\n",
        "        W1=W1\n",
        "    )\n",
        "\n",
        "    # 4. UPDATE WEIGHTS\n",
        "    W3 = W3 - (alpha * dC_dW3)\n",
        "    W2 = W2 - (alpha * dC_dW2)\n",
        "    W1 = W1 - (alpha * dC_dW1)\n",
        "\n",
        "    b3 = b3 - (alpha * dC_db3)\n",
        "    b2 = b2 - (alpha * dC_db2)\n",
        "    b1 = b1 - (alpha * dC_db1)\n",
        "\n",
        "\n",
        "    if e % 20 == 0:\n",
        "      print(f\"epoch {e}: cost = {error:4f}\")\n",
        "\n",
        "  return costs"
      ],
      "metadata": {
        "id": "Szyn5dHdu5Ma"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}